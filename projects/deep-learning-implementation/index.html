<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Deep Learning with matrices | Fernando Borrero Granell</title> <meta name="author" content="Fernando Borrero Granell"> <meta name="description" content="Personal implementation of a neural network trained for computer vision classification."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L8BL6VYKDK"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L8BL6VYKDK");</script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://fbgranell.github.io/projects/deep-learning-implementation/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fernando </span>Borrero Granell</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About me</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Learning with matrices</h1> <p class="post-description">Personal implementation of a neural network trained for computer vision classification.</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f2/header.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="header" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="table-of-contents"> <p style="font-size: 1.75rem; margin-bottom: .5rem;">Table of Contents:</p> <ul> <li style="list-style-type: decimal;"><a href="#section1" style="font-size: 1.30rem; color:#2B2E4A">Introduction.</a></li> <li style="list-style-type: decimal;"><a href="#section2" style="font-size: 1.30rem; color:#2B2E4A">Theory.</a></li> <li style="list-style-type: decimal;"><a href="#section3" style="font-size: 1.30rem; color:#2B2E4A">Results.</a></li> <li style="list-style-type: decimal;"><a href="#section4" style="font-size: 1.30rem; color:#2B2E4A">References.</a></li> <li style="list-style-type: decimal;"><a href="#section5" style="font-size: 1.30rem; color:#2B2E4A">License.</a></li> </ul> </div> <p><a id="section1"></a></p> <h3 id="1-introduction">1. Introduction</h3> <p>Neural networks are widely regarded as one of the most powerful tools in the field of artificial intelligence. However, building such networks can be a daunting task, and that’s why people typically rely on third-party, sophisticated libraries like Tensorflow to reuse other people’s work and streamline the process.</p> <p>In this particular side project, I took on the challenge of creating a neural network completely from scratch, without any assistance from machine learning libraries. This meant writing all the code myself and relying solely on numpy matrices. Thanks to all of this work, I was able to gain a deeper understanding of the fundamental workings of neural networks.</p> <p>You can check the code <a href="https://github.com/fbgranell/deep-learning-implementation" rel="external nofollow noopener" target="_blank">here</a>.</p> <p><a id="section2"></a></p> <h3 id="2-theory">2. Theory</h3> <p>In this section, I will provide an explanation of the theoretical framework of this project. This will involve deriving the mathematical expressions that are used in creating neural networks.</p> <p>In order to train the model, we need to evaluate its predictions on some data. To achieve this, we use a cost function. We’re working with a binary classification problem, thus we use the cross-entropy loss function.</p> \[L(a^{[L]},y) = -\left[ y \ln (a^{[L]}) + (1-y) \ln (1-a^{[L]})\right]\] <p>where \(a^{[L]}\) is the activation value of the output layer. In order to define the neural network we need its parameters, which are stored in matrices \(W^{[l]}\) and vectors \(b^{[l]}\).</p> \[W^{[l]}= \begin{pmatrix} \leftarrow \vec{w}_1^{[l]} \rightarrow \\ \leftarrow \vec{w}_2^{[l]}\rightarrow \\ \vdots \\ \leftarrow \vec{w}_{n^{[l]}}^{[l]}\rightarrow \\ \end{pmatrix} \qquad b^{[l]}= \begin{pmatrix} b_1^{[l]} \\ b_2^{[l]} \\ \vdots \\ b_{n^{[l]}}^{[l]} \\ \end{pmatrix}\] <p>We compute the activation values through forward propagation while using the former parameters.</p> \[\vec{a}^{[l]} = g^{[l]}(\vec{z}^{[l]}) = g^{[l]}(W^{[l]}\cdot \vec{a}^{[l-1]}+\vec{b}^{[l]})\] <p>This expression can also be written for a specific unit \(u\) of the layer \(l\) as.</p> \[a^{[l]}_u = g^{[l]} \left( \sum_{j=1}^{n^{[l-1]}} w_{u,j}^{[l]} a_j^{[l-1]}+b_u^{[l]} \right)\] <h4 id="21-layer-l">2.1 Layer <em>L</em> </h4> <p>The output layer is the easiest to solve since it works with one single neuron. If we apply a bit of calculus, we can compute the derivative of the loss function as</p> \[\frac{\partial L}{\partial a^{[L]}} = L'(a^{[L]}) = -\frac{y}{a^{[L]}}+\frac{1-y}{1-a^{[L]}}\] <p>The output layer is working with a sigmoid activation function, thus \(a^{[L]}=g(z^{[L]})= \frac{1}{1+e^{-z^{[L]}}}\). We can compute its derivate by applying the chain rule from calculus</p> \[g'(z^{[L]})=\frac{(-1)(-1)e^{-z^{[L]}}}{(1+e^{-z^{[L]}})^2}=\frac{(1+e^{-z^{[L]}})-1}{(1+e^{-z^{[L]}})^2}= \frac{1}{1+e^{-z^{[L]}}}-\frac{1}{(1+e^{-z^{[L]}})^2}=a^{[L]}(1-a^{[L]})\] <p>Now we are ready to easily compute the derivate from respect to \(z^{[L]}\) as follows</p> \[\frac{\partial L}{\partial z^{[L]}} = \frac{\partial L}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}} = \left( -\frac{y}{a^{[L]}}+\frac{1-y}{1-a^{[L]}} \right) a^{[L]}(1-a^{[L]})\] \[= -y(1-a^{[L]})+(1-y)a^{[L]}=-y+a^{[L]}y+a^{[L]}-a^{[L]}y=a^{[L]}-y\] <p>The last gradient computed for the layer is those with respect to the parameters, which we will need to perform gradient descent.</p> \[\frac{\partial L}{\partial w^{[L]}_j} = \frac{\partial L}{\partial z^{[L]}} \frac{\partial z^{[L]}}{w^{[L]}_j} = \frac{\partial L}{\partial z^{[L]}} a_j ^{[L-1]} \Rightarrow \frac{\partial L}{\partial\vec{w}^{[L]}} \equiv \frac{\partial L}{\partial z^{[L]}} \vec{a} ^{[L-1]T}\] \[\frac{\partial L}{\partial b^{[L]}} = \frac{\partial L}{\partial z^{[L]}} \frac{\partial z^{[L]}}{b^{[L]}} = \frac{\partial L}{\partial z^{[L]}}\] <h4 id="22-layer-l-1">2.2 Layer <em>L-1</em> </h4> <p>To keep derivating now we have to go back to the previous layer (where we have \(n^{[L-1]}\) units).</p> \[\frac{\partial L}{\partial a_u^{[L-1]}} = \frac{\partial L}{\partial z^{[L]}} \frac{\partial z^{[L]}}{\partial a_u^{[L-1]}} = \frac{\partial L}{\partial z^{[L]}} w^{[L]}_u \Rightarrow \frac{\partial L}{\partial \vec{a}^{[L-1]}} \equiv \frac{\partial L}{\partial z^{[L]}} \vec{w}^{[L]T}\] <p>Now the derivate with respect to \(z\) is computed, where the hidden activation functions \(g\) are ReLUs.</p> \[\frac{\partial L}{\partial z_u^{[L-1]}} = \frac{\partial L}{\partial a_u^{[L-1]}} \frac{\partial a_u^{[L-1]}}{\partial z_u^{[L-1]}} = \frac{\partial L}{\partial a_u^{[L-1]}}g'(z_u^{[L-1]}) \Rightarrow \frac{\partial L}{\partial\vec{z}^{[L-1]}} \equiv \frac{\partial L}{\partial\vec{a}^{[L-1]}} * g'(\vec{z}^{[L-1]})\] <p>As in the previous layer, the last step in this layer is to compute the gradients with respect to the parameters.</p> \[\frac{\partial L}{\partial w^{[L-1]}_{u,q}} = \frac{\partial L}{\partial z_u^{[L-1]}} \frac{\partial z_u^{[L-1]}}{\partial w^{[L-1]}_{u,q}} = \frac{\partial L}{\partial z_u^{[L-1]}} a_q^{[L-2]} \Rightarrow \frac{\partial L}{\partial W^{[L-1]}} = \frac{\partial L}{\partial\vec{z}^{[L-1]}} \vec{a}^{[L-2]T}\] \[\frac{\partial L}{\partial b^{[L-1]}_{u}} = \frac{\partial L}{\partial z_u^{[L-1]}} \frac{\partial z_u^{[L-1]}}{\partial b^{[L-1]}_{u}} = \frac{\partial L}{\partial z_u^{[L-1]}} 1\Rightarrow \frac{\partial L}{\partial\vec{b}^{[L-1]}} = \frac{\partial L}{\partial\vec{z}^{[L-1]}}\] <h4 id="23-layer-l-2">2.3 Layer <em>L-2</em>.</h4> <p>This step is a bit more complex than the previous one \(L \rightarrow L-1\), since we go from one layer with \(n^{[L-1]}\) units to one with \(n^{[L-2]}\) neurons.</p> \[\frac{\partial L}{\partial a_u^{[L-2]}} = \sum_{p=1}^{n^{[L-1]}} \frac{\partial L}{\partial z_p^{[L-1]}} \frac{\partial z_p^{[L-1]}}{\partial a_u^{[L-2]}} = \sum_{p=1}^{n^{[L-1]}} \frac{\partial L}{\partial z_p^{[L-1]}} W_{p,u}^{[L-1]} \Rightarrow \frac{\partial L}{\partial\vec{a}^{[L-2]}} = W^{[L-1]T} \frac{\partial L}{\partial\vec{z}^{[L-1]}}\] <p>With which we can easily compute the derivate with respect to the linear activation values.</p> \[\frac{\partial L}{\partial z_u^{[L-2]}} = \frac{\partial L}{\partial a_u^{[L-2]}} \frac{\partial a_u^{[L-2]}}{dz_u^{[L-2]}}= \frac{\partial L}{\partial a_u^{[L-2]}} g'(z_u^{[L-2]}) \Rightarrow \frac{\partial L}{\partial\vec{z}^{[L-2]}} = W^{[L-1]T} \frac{\partial L}{\partial\vec{z}^{[L-1]}}*g'(\vec{z}^{[L-2]})\] <p>Now we could compute the derivates with respect to the parameters, but the formulas would be the same we’ve already seen. Let’s write it in a more general way in the next section to get a broader understanding.</p> <h4 id="24-layer-l">2.4 Layer <em>l</em>.</h4> <p>We can generalize our equations for a layer \(l&lt;L\) according to the following expressions.</p> \[\frac{\partial L}{\partial\vec{z}^{[l]}} = W^{[l+1]T} \frac{\partial L}{\partial\vec{z}^{[l+1]}}*g'(\vec{z}^{[l]}), \quad \frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial\vec{z}^{[l]}} \vec{a}^{[l-1]T}, \quad \frac{\partial L}{\partial\vec{b}^{[l]}} = \frac{\partial L}{\partial\vec{z}^{[l]}}\] <p>However, we still need to vectorize them. Our formulas are written for a single sample, but in reality we will have \(m\) of them. In fact, the cost function will be the average of the loss function from each sample, that is:</p> \[J(A^{[L]},Y) = \frac{1}{m} \sum_{i=1}^m L(a^{[L]i},y^i)\] <p>The previous equations can be computed in the same way by averaging through vectorization.</p> \[\frac{\partial L}{\partial Z^{[l]}} = W^{[l+1]T} \frac{\partial L}{\partial Z^{[l+1]}}*g'(Z^{[l]}), \quad \frac{\partial L}{\partial W^{[l]}} =\frac{1}{m} \frac{\partial L}{\partial Z^{[l]}} \vec{A}^{[l-1]T}, \quad \frac{\partial L}{\partial\vec{b}^{[l]}} = \frac{1}{m} \sum_{i=1}^m \frac{\partial L}{\partial\vec{z}^{[l]i}}\] <p>We are now prepared to build the model, train it using gradient descent by employing forward and backward propagation techniques, and ultimately utilize it to make predictions.</p> <p><a id="section3"></a></p> <h3 id="3-results">3. Results</h3> <p>I trained my neural network on a small dataset of over 250 images containing some cat and non-cat pictures.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f2/cats_preview.png" class="img-fluid rounded" width="auto" height="auto" title="Preview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The algorithm worked as expected and so the model was succesfully trained by it. The cost function decreases in each iteration of gradient descent towards a global minimum, as we can see in the plot below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f2/cost.png" class="img-fluid rounded" width="auto" height="auto" title="Cost evolution" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The model was then evaluated on a test dataset, where it achieved an accuracy of 82%. While it’s true that we could potentially improve the performance of the model by utilizing a larger dataset and adjusting the model’s architecture, we’re limited to using only 250 photos for this project. Fortunately, this quantity is sufficient for the project’s objectives. You can see the results here:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f2/cats_prediction.png" class="img-fluid rounded" width="auto" height="auto" title="Predictions" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a id="section4"></a></p> <h3 id="4-references">4. References</h3> <p>The images used in this project were obtained from the <strong>catvnoncat</strong> dataset available on <a href="https://www.kaggle.com/datasets/muhammeddalkran/catvnoncat" rel="external nofollow noopener" target="_blank">Kaggle</a>. This dataset contains 209 images for training, and 50 images for testing, all in color.</p> <p><a id="section5"></a></p> <h3 id="5-license">5. License</h3> <p>This project is intended for educational purposes only and all materials, including but not limited to code, data, and documentation, are available for free use, modification, and distribution.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Fernando Borrero Granell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>