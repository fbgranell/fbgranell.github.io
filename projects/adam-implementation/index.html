<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Building Adam Optimization | Fernando Borrero Granell</title> <meta name="author" content="Fernando Borrero Granell"> <meta name="description" content="Implementation of the Adaptive Moment Estimation (or Adam) optimizer in deep learning."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L8BL6VYKDK"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L8BL6VYKDK");</script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://fbgranell.github.io/projects/adam-implementation/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fernando </span>Borrero Granell</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About me</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building Adam Optimization</h1> <p class="post-description">Implementation of the Adaptive Moment Estimation (or Adam) optimizer in deep learning.</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f3/header.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="header" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="table-of-contents"> <p style="font-size: 1.75rem; margin-bottom: .5rem;">Table of Contents:</p> <ul> <li style="list-style-type: decimal;"><a href="#section1" style="font-size: 1.30rem; color:#2B2E4A">Introduction.</a></li> <li style="list-style-type: decimal;"><a href="#section2" style="font-size: 1.30rem; color:#2B2E4A">Theory.</a></li> <li style="list-style-type: decimal;"><a href="#section3" style="font-size: 1.30rem; color:#2B2E4A">Results.</a></li> <li style="list-style-type: decimal;"><a href="#section4" style="font-size: 1.30rem; color:#2B2E4A">License.</a></li> </ul> </div> <p><a id="section1"></a></p> <h3 id="1-introduction">1. Introduction</h3> <p>In this side project, I tackled the challenge of creating an implementation of the Adam optimizer completely from scratch, without relying on pre-existing machine learning libraries (this involved writing all the code myself). While many popular deep learning libraries offer sophisticated optimization algorithms like Adam, implementing them from scratch can provide a deeper understanding of their inner workings.</p> <p>Adam (short for Adaptive Moment Estimation) is a widely used optimization algorithm for deep learning that combines the benefits of two other popular optimization methods, Adagrad and RMSprop. The main goal of Adam is to improve the efficiency and speed of gradient-based optimization algorithms.</p> <p>You can check the code <a href="https://github.com/fbgranell/adam-implementation" rel="external nofollow noopener" target="_blank">here</a>.</p> <p><a id="section2"></a></p> <h3 id="2-theory">2. Theory</h3> <p>This section will explain the inner workings of three optimization algorithms: Batch gradient descent, Mini-batch gradient descent, and Adam’s algorithm.</p> <h4 id="21-mini-batch-gradient-descent">2.1 Mini-batch gradient descent.</h4> <p>Unlike batch gradient descent, which updates the parameters using the gradients computed on the entire training dataset, mini-batch gradient descent updates the parameters using a subset, or “mini-batch,” of the training data at each iteration. This approach can be more efficient and practical for training large datasets, as it allows the algorithm to make progress with fewer computations and reduces memory requirements.</p> <p>In this project I chose a \(\text{mini-batch-size}=64\) (using a power of two is necessary for performance reasons.)</p> <h4 id="22-weighted-exponential-averages">2.2 Weighted exponential averages.</h4> <p>A weighted exponential average is a statistical technique commonly used for calculating a smoothed average of a set of values by placing greater weight on recent observations while still considering past ones. The recursive formula is</p> \[V_t=\beta V_{t-1}+(1-\beta)\theta_t\] <p>If we apply the formula recursively, we obtain</p> \[V_t=\beta (\beta V_{t-2}+(1-\beta)\theta_{t-1})+(1-\beta)\theta_t = \beta^2V_{t-2}+\beta(1-\beta)\theta_{t-1}+(1-\beta)\theta_t =\] \[= \beta^2 (\beta V_{t-3}+(1-\beta)\theta_{t-2})+\beta(1-\beta)\theta_{t-1}+(1-\beta)\theta_t =\] \[\beta^3V_{t-3}+\beta^2(1-\beta)\theta_{t-2}+\beta(1-\beta)\theta_{t-1}+(1-\beta)\theta_t\] <p>We can see that the formula generalizes to the expression below:</p> \[V_t = \sum_{i=1}^t \beta^{t-i}(1-\beta) \theta_i\] <p>The value of \(V_t\) is a weighted average, where the weight of the measurement \(\theta_i\) decays exponentially as we get away from the current value \(t\), and that is why it’s called exponential weighted average. However this mean is not normalized for \(\theta\), and that is where the <strong>bias term correction</strong> comes into play:</p> \[V_{t,corrected} = \frac{V_t}{1-\beta^t}\] <p>We have seen that this method assigns exponentially decreasing weights to the past observations, with the weights determined by a smoothing factor or decay rate that controls how quickly the weights decrease over time. The weighted exponential average can be useful for reducing the impact of noise or fluctuations in data while preserving important patterns or trends.</p> <h4 id="23-adaptive-moment-estimation-adam-algorithm">2.3 Adaptive Moment Estimation (Adam) algorithm.</h4> <p>Adam algorithm uses the power of weighted exponential averages twice. Instead of taking steps proportional to the current gradient, Adam uses the first moment (weighted exponential average of the gradients). Thanks to this, gradients in opposite directions (oscillations) will cancel each other out, and we’ll increase convergence. This technique then divides the first moment by the root square of the second moment (weighted exponential average of the squared gradients), with works as a normalizer.</p> <p>We can compute everything by integrating this code in our <em>update_parameters</em> step of gradient descent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">On</span> <span class="n">iteration</span> <span class="n">t</span><span class="p">:</span>
    <span class="c1"># Computing the first moment.
</span>    <span class="n">VdW</span> <span class="o">=</span> <span class="n">b1</span><span class="o">*</span><span class="n">VdW</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="p">)</span><span class="o">*</span><span class="n">dW</span>
    <span class="n">Vdb</span> <span class="o">=</span> <span class="n">b1</span><span class="o">*</span><span class="n">vdb</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="p">)</span><span class="o">*</span><span class="n">dW</span>
    
    <span class="c1"># Computing the second moment.
</span>    <span class="n">SdW</span> <span class="o">=</span> <span class="n">b2</span><span class="o">*</span><span class="n">SdW</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="p">)</span><span class="o">*</span><span class="n">dW</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">Sdb</span> <span class="o">=</span> <span class="n">b2</span><span class="o">*</span><span class="n">Sdb</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="p">)</span><span class="o">*</span><span class="n">db</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Correcting the moments with bias term.
</span>    <span class="n">VdW_cor</span> <span class="o">=</span> <span class="n">VdW</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
    <span class="n">Vdb_cor</span> <span class="o">=</span> <span class="n">Vdb</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
    <span class="n">SdW_cor</span> <span class="o">=</span> <span class="n">SdW</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
    <span class="n">Sdb_cor</span> <span class="o">=</span> <span class="n">Sdb</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># Gradient descent step.
</span>    <span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">Vdw_cor</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">SdW_cor</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">Vdb_cor</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">Sdb_cor</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span>
</code></pre></div></div> <h4 id="23-learning-rate-decay">2.3 Learning rate decay.</h4> <p>If we’re working with mini-batch, we will have some noise in each iteration. Nevertheless, the average direction is the same as the gradient, and at the end we will oscillate around the minimum. We can solve this final oscilation by slowly reducing the learning rate. In this case, I have chosen to work with the following decay:</p> \[\alpha = \frac{1}{1+\text{decayRate}\times \text{epochNumber}}\alpha_0\] <p>Instead of decaying the learning rate in each epoch, it’s possible to reduce it after certain intervals. I chose to work with a decay_interval of 5, which means the rate decays every 5 epochs.</p> <p><a id="section3"></a></p> <h3 id="3-results">3. Results</h3> <p>In this section, we will compare the performance of three different optimization algorithms for classifying a “two-moons” dataset.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f3/dataset.png" class="img-fluid rounded" width="auto" height="auto" title="Preview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Examining the training curves, it becomes evident that <strong>Adam’s algorithm is the most efficient</strong>, converging in just a few epochs. Mini-batch gradient descent follows closely behind, but is not as efficient. Finally, we have batch gradient descent, which is significantly less efficient and unable to reach a satisfactory solution within the same number of epochs.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f3/compare_training.png" class="img-fluid rounded" width="auto" height="auto" title="Training for each method" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The accuracy of the model is much higher for Adam and Mini-batch gradient descent, at around 99%, compared to batch gradient descent’s 86%. Although Adam and Mini-batch are both highly accurate, Adam emerges as the clear winner, achieving the most optimal solution five times faster.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/f3/compare_boundary.png" class="img-fluid rounded" width="auto" height="auto" title="Accuracy for each model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In conclusion, the results of our comparison of optimization algorithms demonstrate the importance of carefully selecting the right method for the given problem. While batch gradient descent may be suitable for small datasets or problems with relatively simple optimization landscapes, Adam and mini-batch gradient descent are more efficient and better suited for larger datasets and more complex problems. Understanding the trade-offs and benefits of each optimization algorithm is essential for achieving optimal performance and training efficiency.</p> <p><a id="section4"></a></p> <h3 id="4-license">4. License</h3> <p>This project is intended for educational purposes only and all materials, including but not limited to code, data, and documentation, are available for free use, modification, and distribution.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Fernando Borrero Granell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>